{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "import pdfplumber\n",
    "from IPython.display import display, HTML, Javascript\n",
    "import ipywidgets as widgets\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "from IPython.display import clear_output\n",
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeData():\n",
    "    \"\"\"\n",
    "    Python tool to scrap text data from different languages. 3 options available:\n",
    "    \n",
    "    - scrape from link: provided with a link, the tool scrapes through the text body from the webpage \n",
    "      corresponding to the link.\n",
    "      \n",
    "    - scrape from keyword: provided with a keyword, the tool performs a google search and retrieves text from \n",
    "      a priority domain webpage(eg: wikipedia) or top google search result.\n",
    "      \n",
    "    - scrape from document: provided with a pdf document, the tools accesses the text using pdfplumber \n",
    "      python package. Note that some portion of the text may not be returned properly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise parameters for google search and priority website \n",
    "        \"\"\"\n",
    "        \n",
    "        self.parser = 'html.parser'\n",
    "        self.tld = 'co.in'\n",
    "        self.search_num = 10\n",
    "        self.search_stop = 10\n",
    "        self.pause = 2\n",
    "        self.search_priority = 'wikipedia'\n",
    "        self.auto_return_index = 0\n",
    "    \n",
    "    def read_from_link(self, link, replace_list=['\\n']):\n",
    "        \"\"\"\n",
    "        This function accesses the text content from a webpage link using beautiful soup. To clean the text, \n",
    "        provide the list of charecters to be removed in replace_list.\n",
    "        \"\"\"\n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.content, self.parser)\n",
    "        data = []\n",
    "        for i in range(len(soup.find_all('p'))):\n",
    "            text = soup.find_all('p')[i].get_text()\n",
    "            for j in range(len(replace_list)):\n",
    "                text = text.replace(replace_list[j],'')\n",
    "            if len(text)>0:\n",
    "                data.append(text)\n",
    "        return data\n",
    "    \n",
    "    def google_search(self, search_keyword, priority=None):\n",
    "        \"\"\"\n",
    "        This function performs google search on the input keyword. Priority can be provided to a particular \n",
    "        website (Eg: wikipedia) \n",
    "        \"\"\"\n",
    "        search_links = []\n",
    "        for link in search(search_keyword, \n",
    "                           tld=self.tld, \n",
    "                           num=self.search_num, \n",
    "                           stop=self.search_stop, \n",
    "                           pause=self.pause):\n",
    "            search_links.append(link)\n",
    "            \n",
    "            if priority is not None:\n",
    "                if self.search_priority in link:\n",
    "                    return link\n",
    "        return search_links[self.auto_return_index]\n",
    "                \n",
    "    \n",
    "    def read_from_doc(self, document):\n",
    "        \"\"\"\n",
    "        This function extracts text from pdf using pdfplumber tool.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        with pdfplumber.open(document) as pdf:\n",
    "            for i in range(len(pdf.pages)):\n",
    "                page = pdf.pages[i]\n",
    "                text = page.extract_text()\n",
    "                if text is not None:\n",
    "                    text = text.replace('\\n', ' ')\n",
    "                    data.append(text) \n",
    "        return data\n",
    "    \n",
    "    def read_page(self, search_keyword=None, link=None, document=None):\n",
    "        if search_keyword == link == document == None:\n",
    "            raise Exception('Provide link, keyword or document to scrape from')\n",
    "            \n",
    "        if search_keyword is not None:\n",
    "            keyword_link = self.google_search(search_keyword, priority=self.search_priority)\n",
    "            text = self.read_from_link(keyword_link)\n",
    "            return(text)\n",
    "            \n",
    "        if link:\n",
    "            text = self.read_from_link(link)\n",
    "            return(text)\n",
    "                \n",
    "        if document:\n",
    "            text = self.read_from_doc(document)\n",
    "            print(f'{len(text)} pages found')\n",
    "            return(text)\n",
    "            \n",
    "scrape_tool = ScrapeData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read from english website</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool.read_page(link='https://en.wikipedia.org/wiki/Agriculture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read from english keyword</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool.read_page(search_keyword='agriculture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read from english pdf document</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool.read_page(document='document_name.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read from list of links with GUI</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveScrappingGUI():\n",
    "    def __init__(self, links):\n",
    "        if not isinstance(links, list):\n",
    "            raise Exception('link input shoudl be present as a python list')\n",
    "        self.linkIdx = 0\n",
    "        self.links = links\n",
    "        self.approvedList = {}\n",
    "        self.storedText = {}\n",
    "        self.defaultColor  = '#EEEEEE'\n",
    "        self.approvedColor = 'lightgreen'\n",
    "        self.rejectedColor = '#FF4500'\n",
    "        self.nextButton = widgets.Button(description = 'Next')\n",
    "        self.approve = widgets.Button(description = 'Approve', disabled=True)\n",
    "        self.reject = widgets.Button(description = 'Reject', disabled=True)\n",
    "        self.nextButton.add_class(\"red_label\")\n",
    "        self.approve.add_class(\"red_label\")\n",
    "        self.reject.add_class(\"red_label\")\n",
    "        self.output = widgets.Output()\n",
    "        display(widgets.HBox((self.nextButton, self.approve,self.reject )), self.output, \n",
    "        HTML(\"<style>.red_label { font-weight: bold}</style>\"),\n",
    "        HTML(\"<style>.red_label { font-family:calibri}</style>\"),\n",
    "        HTML(\"<style>.red_label { font-size:16px}</style>\"))\n",
    "        \n",
    "    def on_button_clicked_approve(self, b):\n",
    "        with self.output:\n",
    "            self.nextButton.disabled = False\n",
    "            self.approve.style.button_color = self.approvedColor\n",
    "            self.reject.style.button_color = self.defaultColor\n",
    "            self.approvedList[self.links[self.linkIdx-1]] = True\n",
    "                \n",
    "    def on_button_clicked_reject(self, b):\n",
    "        with self.output:\n",
    "            self.nextButton.disabled = False\n",
    "            self.approve.style.button_color = self.defaultColor\n",
    "            self.reject.style.button_color = self.rejectedColor\n",
    "            self.approvedList[self.links[self.linkIdx-1]] = False\n",
    "                \n",
    "    def on_button_clicked(self, b):\n",
    "        with self.output:\n",
    "            if  self.linkIdx == len(self.links):\n",
    "                clear_output()\n",
    "                print('All links visited. Approved link can be accessed with \"gui.approvedList\"')\n",
    "                self.reject.disabled = True\n",
    "                self.approve.disabled = True\n",
    "                self.nextButton.disabled = True\n",
    "                self.reject.style.button_color = self.defaultColor\n",
    "                self.approve.style.button_color = self.defaultColor\n",
    "            else:\n",
    "                clear_output()\n",
    "                self.reject.disabled = True\n",
    "                self.approve.disabled = True\n",
    "                self.nextButton.disabled = True\n",
    "                self.reject.style.button_color = self.defaultColor\n",
    "                self.approve.style.button_color = self.defaultColor\n",
    "                print('Extracting..')\n",
    "                text = scrape_tool.read_page(self.links[self.linkIdx])\n",
    "                clear_output()\n",
    "                self.storedText[self.links[self.linkIdx]] = text\n",
    "                for para in text:\n",
    "                    print(para, '\\n')\n",
    "                webbrowser.open(self.links[self.linkIdx])\n",
    "                self.reject.disabled = False\n",
    "                self.approve.disabled = False\n",
    "                self.linkIdx += 1\n",
    "            \n",
    "    def start(self):\n",
    "        self.approve.on_click(self.on_button_clicked_approve)\n",
    "        self.reject.on_click(self.on_button_clicked_reject)\n",
    "        self.nextButton.on_click(self.on_button_clicked)\n",
    "    \n",
    "    def save(self, path):\n",
    "        savedSet = set()\n",
    "        for key, item in self.approvedList.items():\n",
    "            if item == True:\n",
    "                saveName = key.replace('/', '_')\n",
    "                if saveName in savedSet:\n",
    "                    raise Exception(f'{key} already saved, conflicitng links present. Report it to sathvikudupa66@gmail.com')\n",
    "                else:\n",
    "                    savedSet.add(saveName)\n",
    "                try:\n",
    "                    with open(os.path.join(path, saveName), 'w') as f:\n",
    "                        f.writelines(self.storedText[key])\n",
    "                except:\n",
    "                    raise Exception(f'Unable to save extracted text from link {key}. Please verify if \"{path}\" exists')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = ['https://en.wikipedia.org/wiki/Agriculture', 'https://www.bbc.com/hindi/india-56901831',\n",
    "        'https://en.wikipedia.org/wiki/Main_Page']\n",
    "\n",
    "#load links from csv into a list\n",
    "gui = ActiveScrappingGUI(links)\n",
    "gui.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gui.approvedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store this as .pickle if rejected links are processed at a later time.\n",
    "linkStatusPath = 'status.pickle'\n",
    "with open(linkStatusPath, 'wb') as handle:\n",
    "    pickle.dump(gui.approvedList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save approved text </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide your folder path\n",
    "gui.save(path='saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example on how to proceed with rejected links (Work In Progress)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class HandleRejects():\n",
    "    def __init__(self, storedGUIStatusDict, openWebPage = True, ):\n",
    "        self.approve = widgets.Button(description = 'Approve Changes')\n",
    "        self.refresh = widgets.Button(description = 'Refresh Changes')\n",
    "        self.discard = widgets.Button(description = 'Unable to process')\n",
    "        self.output = widgets.Output()\n",
    "        self.openWebPage = openWebPage\n",
    "        self.defaultColor  = '#EEEEEE'\n",
    "        self.approve.disabled = True\n",
    "        self.discard.disabled = True\n",
    "        self.approve.add_class(\"layout\")\n",
    "        self.refresh.add_class(\"layout\")\n",
    "        self.discard.add_class(\"layout\")\n",
    "        with open(linkStatusPath, 'rb') as handle:\n",
    "            self.statusDct = pickle.load(handle)\n",
    "        self.resolvedLinks = {}\n",
    "        self.discardedLinks = {}\n",
    "        self.initalVal = None\n",
    "        self.initalVal = self.reportStatus()\n",
    "        self.linkIdx = 0\n",
    "        self.visitSet = set()\n",
    "        self.linkFromStatus = [key for key, item in self.statusDct.items() if item == False]\n",
    "        \n",
    "    def reportStatus(self):\n",
    "        if self.initalVal == None:\n",
    "            toBeResolved = sum([1 if item == False else 0 for key, item in self.statusDct.items()])\n",
    "            return toBeResolved\n",
    "        else:\n",
    "            self.remainingCount = self.initalVal - len(self.resolvedLinks) - len(self.discardedLinks)\n",
    "            print(f'{self.remainingCount}/{self.initalVal} links left to be resolved')\n",
    "    \n",
    "    def on_button_clicked_approve(self, b):\n",
    "        with self.output:\n",
    "            self.approve.style.button_color = self.defaultColor\n",
    "            self.resolvedLinks[self.currentLink] = self.currentText\n",
    "            self.linkIdx += 1 \n",
    "            clear_output()\n",
    "            print(f'Text extracted from {self.currentLink} approved')\n",
    "            self.reportStatus()\n",
    "            self.discard.disabled = True\n",
    "            self.approve.disabled = True\n",
    "    \n",
    "    def on_button_clicked_discard(self, b):\n",
    "        with self.output:\n",
    "            self.discard.style.button_color = self.defaultColor\n",
    "            self.discardedLinks[self.currentLink] = self.currentText\n",
    "            self.linkIdx += 1 \n",
    "            clear_output()\n",
    "            print(f'Text extracted from {self.currentLink} needs manual inspection. Unable to proceed with web scraping code')\n",
    "            self.reportStatus()\n",
    "            self.approve.disabled = True\n",
    "            self.discard.disabled = True\n",
    "            \n",
    "    def on_button_clicked_refresh(self, b):\n",
    "        with self.output:\n",
    "            if len(self.linkFromStatus) == 0:\n",
    "                raise Exception('No sentences dound in dict')\n",
    "            self.approve.disabled = False\n",
    "            self.discard.disabled = False\n",
    "            self.reportStatus()\n",
    "            print('Extracting..')\n",
    "            if self.linkIdx >= len(self.linkFromStatus):\n",
    "                clear_output()\n",
    "                self.discard.disabled = True\n",
    "                self.approve.disabled = True\n",
    "                self.refresh.disabled = True\n",
    "                raise Exception('All links visited. : )')\n",
    "            self.currentLink = self.linkFromStatus[self.linkIdx]\n",
    "            if self.currentLink not in self.resolvedLinks.keys():\n",
    "                if self.currentLink not in self.visitSet:\n",
    "                    self.visitSet.add(self.currentLink)\n",
    "                    webbrowser.open(self.currentLink)\n",
    "                self.currentText = self.readFn(self.currentLink)\n",
    "                clear_output()\n",
    "                for line in self.currentText:\n",
    "                    print(line, '\\n')\n",
    "    \n",
    "    def start(self, readFn):\n",
    "        display(widgets.HBox((self.approve, self.refresh, self.discard)), self.output, \n",
    "        HTML(\"<style>.layout { font-weight: bold}</style>\"),\n",
    "        HTML(\"<style>.layout { font-family:calibri}</style>\"),\n",
    "        HTML(\"<style>.layout { font-size:16px}</style>\"))\n",
    "        self.readFn = readFn\n",
    "        self.refresh.on_click(self.on_button_clicked_refresh)\n",
    "        self.approve.on_click(self.on_button_clicked_approve)\n",
    "        self.discard.on_click(self.on_button_clicked_discard)\n",
    "    \n",
    "    def save(self, path):\n",
    "        savedSet = set()\n",
    "        for key, item in self.resolvedLinks.items():\n",
    "            saveName = key.replace('/', '_')\n",
    "            if saveName in savedSet:\n",
    "                raise Exception(f'{key} already saved, conflicitng links present. Report it to sathvikudupa66@gmail.com')\n",
    "            else:\n",
    "                savedSet.add(saveName)\n",
    "            try:\n",
    "                with open(os.path.join(path, saveName), 'w') as f:\n",
    "                    f.writelines(item)\n",
    "            except:\n",
    "                raise Exception(f'Unable to save extracted text from link {key}. Please verify if \"{path}\" exists')\n",
    "    def saveDiscarded(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            data = list(self.discardedLinks.keys())\n",
    "            for line in data:\n",
    "                f.write(line+'\\n')\n",
    "                \n",
    "linkStatusPath = 'status.pickle'\n",
    "handle = HandleRejects(storedGUIStatusDict=linkStatusPath);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f93e982e084dd49994ce2ff4824328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Approve Changes', style=ButtonStyle(), _dom_classes=('layout',)), Button(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dff9c6de8914432b123b9d7a99f9a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'stream', 'text': 'Agriculture \\n\\n\\xa0China \\n\\n\\xa0India \\n\\n\\xa0European Un…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.layout { font-weight: bold}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.layout { font-family:calibri}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.layout { font-size:16px}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_from_link_custom(link, replace_list=['\\n']):  \n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    data = []\n",
    "    for i in range(len(soup.find_all('p'))):\n",
    "        text = soup.find_all('p')[i].get_text()\n",
    "        for j in range(len(replace_list)):\n",
    "            text = text.replace(replace_list[j],'')\n",
    "        if len(text)>0:\n",
    "            data.append(text)\n",
    "    return data  \n",
    "\n",
    "handle.start(readFn=read_from_link_custom1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['https://en.wikipedia.org/wiki/Agriculture', 'https://en.wikipedia.org/wiki/Main_Page'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resolved links with extracted text present here\n",
    "handle.resolvedLinks.keys()\n",
    "#links which need manual inspection present here\n",
    "list(handle.discardedLinks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.save('saveResolved')\n",
    "handle.saveDiscarded('furtherInspection/day1links.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Read from hindi webpage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_tool.read_page(link=\"https://hi.wikipedia.org/wiki/%E0%A4%95%E0%A5%83%E0%A4%B7%E0%A4%BF\")\n",
    "scrape_tool.read_page(link=\"https://www.bbc.com/hindi/india-56901831\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Read from hindi document</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool.read_page(document='RedRidingHood-H-2mb.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Read from kannada webpage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_tool.read_page(link=\"https://kn.wikipedia.org/wiki/%E0%B2%B8%E0%B2%BE%E0%B2%B5%E0%B2%AF%E0%B2%B5_%E0%B2%AC%E0%B3%87%E0%B2%B8%E0%B2%BE%E0%B2%AF\")\n",
    "scrape_tool.read_page(link=\"https://kannada.asianetnews.com/karnataka-districts/bjp-mla-g-somashekara-reddy-talks-lockdown-in-karnataka-grg-qs9n0r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
